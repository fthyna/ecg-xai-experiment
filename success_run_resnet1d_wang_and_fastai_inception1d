(ecg_env) iyafath@iyafath:~/ta/code/ecg_ptbxl_benchmarking/code$ python reproduce_results.py
True
/home/iyafath/anaconda3/envs/ecg_env/bin/python
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         4.183006    #na#        00:06     ██████████████████████████-----------------------| 71.32% [97/136 00:06<00:02 1.7655]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.769864    0.616121    00:11
1         0.476346    0.276532    00:11
2         0.172156    0.106635    00:11
3         0.108551    0.101244    00:11
4         0.095328    0.087172    00:12
5         0.088418    0.084183    00:12
6         0.082039    0.077567    00:12
7         0.077442    0.075122    00:12
8         0.073800    0.072653    00:12
9         0.071797    0.071436    00:12
10        0.070088    0.072521    00:12
11        0.069247    0.068697    00:12
12        0.068411    0.068195    00:12
13        0.067218    0.068213    00:12
14        0.067049    0.068944    00:12
15        0.065028    0.066739    00:12
16        0.064827    0.065820    00:12
17        0.064517    0.066001    00:12
18        0.063850    0.066633    00:12
19        0.062266    0.067539    00:12
20        0.062988    0.065385    00:12
21        0.061778    0.065078    00:12
22        0.061134    0.065233    00:12
23        0.060872    0.063392    00:12
24        0.060814    0.063094    00:12
25        0.059534    0.063728    00:12
26        0.059998    0.062818    00:12
27        0.059051    0.062863    00:12
28        0.058763    0.061911    00:12
29        0.058009    0.062318    00:12
30        0.057845    0.061432    00:12
31        0.057383    0.062230    00:12
32        0.057357    0.061711    00:12
33        0.056334    0.061124    00:12
34        0.055848    0.061772    00:12
35        0.055235    0.061522    00:12
36        0.054943    0.061695    00:12
37        0.054139    0.060871    00:12
38        0.053662    0.060958    00:12
39        0.052861    0.061268    00:12
40        0.052976    0.060622    00:12
41        0.052361    0.060866    00:12
42        0.051775    0.060788    00:12
43        0.051858    0.061665    00:12
44        0.051592    0.061080    00:12
45        0.051548    0.061294    00:12
46        0.051755    0.061133    00:12
47        0.050940    0.061295    00:12
48        0.050786    0.061153    00:12
49        0.050585    0.061170    00:12
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         3.487988    #na#        00:20     ██████████████████████████-----------------------| 71.32% [97/136 00:20<00:08 2.5355]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.767806    0.612208    00:34
1         0.481481    0.291159    00:35
2         0.173962    0.107703    00:35
3         0.106980    0.097503    00:35
4         0.094033    0.094738    00:35
5         0.089016    0.082209    00:35
6         0.082377    0.087057    00:35
7         0.076233    0.079297    00:36
8         0.073506    0.070378    00:35
9         0.070580    0.069658    00:35
10        0.068819    0.070907    00:35
11        0.067806    0.071798    00:35
12        0.066500    0.068058    00:35
13        0.066074    0.068925    00:35
14        0.064785    0.066211    00:35
15        0.063903    0.064642    00:35
16        0.063284    0.065084    00:35
17        0.062771    0.063943    00:35
18        0.062275    0.064297    00:35
19        0.061533    0.064484    00:35
20        0.060927    0.063282    00:35
21        0.060153    0.063900    00:35
22        0.059557    0.062658    00:35
23        0.059333    0.062107    00:36
24        0.058521    0.061411    00:35
25        0.057654    0.063670    00:35
26        0.057983    0.061079    00:35
27        0.056626    0.063680    00:35
28        0.056384    0.063513    00:35
29        0.056550    0.061055    00:36
30        0.054905    0.064584    00:35
31        0.054388    0.062217    00:35
32        0.054163    0.061396    00:36
33        0.054057    0.061496    00:36
34        0.053465    0.060864    00:35
35        0.053270    0.061812    00:35
36        0.051629    0.061222    00:35
37        0.051608    0.061698    00:35
38        0.051358    0.061618    00:35
39        0.050258    0.061651    00:35
40        0.050246    0.061037    00:36
41        0.049316    0.061849    00:35
42        0.049326    0.061397    00:35
43        0.049119    0.062284    00:35
44        0.048141    0.061948    00:36
45        0.048066    0.062698    00:35
46        0.047807    0.061946    00:35
47        0.047880    0.062081    00:35
48        0.047955    0.062506    00:35
49        0.047502    0.061815    00:35
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
fastai_xresnet1d101
naive
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         2.931461    #na#        00:06     ███████████████████████████----------------------| 72.93% [97/133 00:06<00:02 1.8255]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.788813    0.612173    00:11
1         0.497174    0.288168    00:12
2         0.178642    0.099319    00:12
3         0.101458    0.081883    00:12
4         0.085976    0.078710    00:12
5         0.079506    0.073580    00:12
6         0.073725    0.068526    00:12
7         0.070950    0.067435    00:12
8         0.067300    0.066343    00:12
9         0.065014    0.064905    00:12
10        0.064384    0.063696    00:12
11        0.063335    0.063154    00:12
12        0.061534    0.061700    00:12
13        0.060373    0.064443    00:12
14        0.059297    0.059631    00:12
15        0.059216    0.059630    00:12
16        0.058490    0.062172    00:12
17        0.057783    0.058672    00:12
18        0.058000    0.059540    00:12
19        0.057243    0.062049    00:12
20        0.055964    0.059087    00:12
21        0.055096    0.057490    00:12
22        0.054792    0.056764    00:12
23        0.054571    0.057741    00:12
24        0.054254    0.056493    00:13
25        0.053279    0.057169    00:12
26        0.053128    0.057216    00:12
27        0.052377    0.056604    00:12
28        0.050961    0.056450    00:12
29        0.050784    0.056590    00:12
30        0.051020    0.056062    00:12
31        0.050245    0.056069    00:12
32        0.049982    0.055484    00:12
33        0.048884    0.056308    00:12
34        0.048699    0.055264    00:12
35        0.048379    0.055508    00:12
36        0.047591    0.057065    00:12
37        0.047542    0.055391    00:12
38        0.045677    0.055733    00:12
39        0.046298    0.055778    00:12
40        0.045466    0.056106    00:12
41        0.044406    0.056550    00:12
42        0.044097    0.056295    00:12
43        0.043728    0.056398    00:12
44        0.043521    0.056335    00:12
45        0.043479    0.056844    00:12
46        0.042498    0.056702    00:12
47        0.042490    0.056639    00:12
48        0.043052    0.056979    00:12
49        0.042948    0.056788    00:12
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         3.123939    #na#        00:20     ███████████████████████████----------------------| 72.93% [97/133 00:20<00:07 2.3631]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.769684    0.606679    00:33
1         0.477198    0.286008    00:34
2         0.169485    0.116317    00:35
3         0.097574    0.080645    00:35
4         0.082580    0.072554    00:35
5         0.076621    0.072870    00:35
6         0.073722    0.070262    00:35
7         0.069678    0.066324    00:35
8         0.066057    0.064886    00:35
9         0.064738    0.063313    00:35
10        0.063534    0.063044    00:35
11        0.062101    0.067479    00:35
12        0.060066    0.063196    00:35
13        0.060095    0.062247    00:35
14        0.059209    0.063140    00:35
15        0.058484    0.058222    00:35
16        0.058291    0.059088    00:35
17        0.057280    0.060364    00:35
18        0.055868    0.058930    00:35
19        0.056101    0.059869    00:35
20        0.055053    0.059404    00:35
21        0.054285    0.058505    00:35
22        0.053206    0.056497    00:35
23        0.053165    0.056084    00:35
24        0.052221    0.056352    00:35
25        0.051848    0.058058    00:35
26        0.050816    0.057535    00:35
27        0.050204    0.057698    00:35
28        0.049546    0.060742    00:35
29        0.048666    0.056694    00:35
30        0.048397    0.057852    00:35
31        0.047519    0.056720    00:35
32        0.046605    0.057262    00:35
33        0.046115    0.058086    00:35
34        0.045391    0.057053    00:35
35        0.044547    0.058901    00:35
36        0.042725    0.059071    00:35
37        0.042681    0.058120    00:35
38        0.042112    0.058912    00:35
39        0.041375    0.058781    00:35
40        0.040453    0.059276    00:35
41        0.040190    0.059910    00:35
42        0.039417    0.060614    00:35
43        0.038562    0.061101    00:35
44        0.038347    0.062188    00:35
45        0.037908    0.061483    00:35
46        0.037441    0.061555    00:35
47        0.037028    0.062079    00:35
48        0.037455    0.061498    00:35
49        0.037403    0.061953    00:35
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
naive
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         4.158238    #na#        00:06     ██████████████████████████-----------------------| 72.18% [96/133 00:06<00:02 2.7438]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.765480    0.577901    00:11
1         0.501569    0.309610    00:12
2         0.220051    0.144325    00:12
3         0.152318    0.135409    00:12
4         0.134372    0.131608    00:12
5         0.122834    0.114029    00:12
6         0.115090    0.114385    00:12
7         0.110305    0.112735    00:12
8         0.107438    0.112713    00:12
9         0.104838    0.107203    00:12
10        0.103451    0.113175    00:12
11        0.100862    0.104043    00:12
12        0.100174    0.102269    00:12
13        0.098642    0.102438    00:12
14        0.096645    0.102914    00:12
15        0.097116    0.097068    00:12
16        0.094417    0.103930    00:12
17        0.094468    0.097349    00:12
18        0.092278    0.096355    00:12
19        0.091558    0.095687    00:12
20        0.091435    0.093689    00:12
21        0.090737    0.093444    00:12
22        0.087764    0.096168    00:12
23        0.088145    0.100533    00:12
24        0.087755    0.093786    00:12
25        0.086347    0.094692    00:12
26        0.086039    0.092280    00:12
27        0.085590    0.092649    00:12
28        0.083426    0.092939    00:12
29        0.083153    0.095577    00:12
30        0.082024    0.094196    00:12
31        0.080054    0.094181    00:12
32        0.080286    0.091772    00:12
33        0.079230    0.093199    00:12
34        0.078122    0.095360    00:12
35        0.076579    0.093791    00:12
36        0.076019    0.094820    00:12
37        0.074747    0.093114    00:12
38        0.074989    0.093054    00:12
39        0.073622    0.093079    00:12
40        0.071870    0.093911    00:12
41        0.070596    0.094532    00:12
42        0.069242    0.095460    00:12
43        0.069013    0.095595    00:12
44        0.069380    0.096129    00:12
45        0.068079    0.095997    00:12
46        0.067760    0.095757    00:12
47        0.066787    0.096194    00:12
48        0.067092    0.095353    00:12
49        0.066727    0.095776    00:12
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         3.685228    #na#        00:20     ██████████████████████████-----------------------| 72.18% [96/133 00:20<00:07 2.8076]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.778806    0.595014    00:34
1         0.502619    0.313279    00:35
2         0.213776    0.146048    00:35
3         0.146782    0.130000    00:35
4         0.129278    0.119534    00:35
5         0.121150    0.117127    00:35
6         0.114491    0.106037    00:35
7         0.108645    0.109820    00:35
8         0.106412    0.105633    00:35
9         0.102769    0.107400    00:35
10        0.101596    0.114750    00:35
11        0.099995    0.107748    00:35
12        0.099523    0.101564    00:35
13        0.097280    0.109574    00:35
14        0.096984    0.103199    00:35
15        0.095226    0.100481    00:35
16        0.093755    0.096396    00:35
17        0.092491    0.098192    00:35
18        0.092017    0.104982    00:35
19        0.091431    0.095940    00:35
20        0.089278    0.096371    00:35
21        0.088806    0.098552    00:35
22        0.087453    0.099006    00:35
23        0.085696    0.095474    00:35
24        0.083776    0.098472    00:35
25        0.084020    0.095998    00:35
26        0.081982    0.092659    00:35
27        0.081036    0.096020    00:35
28        0.079949    0.097682    00:35
29        0.078256    0.094972    00:35
30        0.077737    0.095009    00:35
31        0.075819    0.096304    00:35
32        0.075637    0.093803    00:35
33        0.073838    0.094660    00:35
34        0.072061    0.094624    00:35
35        0.070174    0.097024    00:35
36        0.068855    0.096362    00:35
37        0.067115    0.098794    00:35
38        0.065868    0.099974    00:35
39        0.065585    0.100985    00:35
40        0.063300    0.101688    00:35
41        0.062441    0.103400    00:35
42        0.062434    0.101131    00:35
43        0.060196    0.102832    00:35
44        0.058816    0.103971    00:35
45        0.057848    0.104782    00:35
46        0.057498    0.104095    00:35
47        0.056805    0.104047    00:35
48        0.057421    0.104012    00:35
49        0.056749    0.105270    00:35
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
naive
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         6.869483    #na#        00:06     ██████████████████████████-----------------------| 71.43% [95/133 00:06<00:02 3.0597]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.645000    0.487804    00:11
1         0.493711    0.378365    00:12
2         0.386644    0.342304    00:12
3         0.344234    0.345217    00:12
4         0.314298    0.311207    00:12
5         0.306435    0.304627    00:12
6         0.294082    0.309435    00:12
7         0.287725    0.303012    00:12
8         0.288652    0.314526    00:12
9         0.285556    0.343880    00:12
10        0.282265    0.307055    00:12
11        0.279629    0.307985    00:12
12        0.275459    0.297149    00:12
13        0.275385    0.287488    00:12
14        0.271255    0.301509    00:12
15        0.267173    0.289289    00:12
16        0.262150    0.283530    00:12
17        0.262118    0.282671    00:13
18        0.258320    0.279308    00:12
19        0.256510    0.301807    00:12
20        0.256343    0.283675    00:12
21        0.247002    0.277089    00:12
22        0.247222    0.298024    00:12
23        0.247395    0.281624    00:12
24        0.241080    0.272910    00:12
25        0.235370    0.280896    00:12
26        0.236179    0.269084    00:12
27        0.233322    0.272318    00:12
28        0.232753    0.270115    00:12
29        0.228863    0.267359    00:12
30        0.227296    0.270931    00:12
31        0.224731    0.268153    00:12
32        0.220131    0.279142    00:12
33        0.218033    0.271371    00:12
34        0.211801    0.274740    00:12
35        0.209353    0.275244    00:12
36        0.207370    0.282496    00:12
37        0.202296    0.276354    00:12
38        0.200619    0.279261    00:12
39        0.196219    0.283177    00:12
40        0.191639    0.285107    00:12
41        0.189766    0.288180    00:12
42        0.187054    0.290095    00:12
43        0.185619    0.290396    00:20
44        0.183085    0.291237    01:14
45        0.181287    0.294178    00:28
46        0.177903    0.292333    00:11
47        0.178815    0.293546    00:11
48        0.177883    0.295889    00:12
49        0.178284    0.292099    00:12
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         4.721273    #na#        00:19     █████████████████████████------------------------| 70.68% [94/133 00:19<00:08 1.5668]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.612472    0.466921    00:33
1         0.459645    0.378575    00:35
2         0.369322    0.342976    00:35
3         0.328247    0.311485    00:35
4         0.312475    0.304620    00:35
5         0.299580    0.314936    00:35
6         0.291126    0.306604    00:35
7         0.284883    0.297705    00:35
8         0.278531    0.294182    00:35
9         0.277273    0.288634    00:35
10        0.276436    0.286509    00:35
11        0.268473    0.307265    00:35
12        0.267370    0.300015    00:35
13        0.266636    0.294560    00:35
14        0.262022    0.288788    00:35
15        0.257593    0.279222    00:35
16        0.256273    0.275318    00:35
17        0.250296    0.286564    00:35
18        0.247064    0.276047    00:35
19        0.245836    0.275254    00:35
20        0.241476    0.291517    00:35
21        0.236669    0.287491    00:35
22        0.235998    0.274684    00:35
23        0.235791    0.272928    00:35
24        0.228955    0.276087    00:35
25        0.226202    0.280970    00:35
26        0.220982    0.278921    00:35
27        0.219853    0.273054    00:35
28        0.217222    0.283448    00:35
29        0.212098    0.280752    00:35
30        0.204509    0.283703    00:35
31        0.198851    0.285820    00:35
32        0.196398    0.295425    00:35
33        0.185319    0.283178    00:35
34        0.183733    0.292845    00:35
35        0.173489    0.296378    00:35
36        0.169267    0.306969    00:35
37        0.159830    0.311257    00:35
38        0.156857    0.323355    00:35
39        0.148455    0.326274    00:35
40        0.143322    0.334969    00:35
41        0.138219    0.341301    00:35
42        0.133742    0.348925    00:35
43        0.127144    0.353473    00:35
44        0.124718    0.366285    00:35
45        0.122228    0.363393    00:35
46        0.121295    0.373697    00:35
47        0.119000    0.369350    00:35
48        0.113329    0.367387    00:35
49        0.116897    0.377085    00:34
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
naive
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         1.008126    #na#        00:04
1         3.267081    #na#        00:02     ████████████████████████-------------------------| 69.64% [39/56 00:02<00:01 1.8420]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.914039    0.737322    00:05
1         0.808399    0.612729    00:05
2         0.662965    0.425383    00:05
3         0.440341    0.224390    00:05
4         0.283722    0.200209    00:05
5         0.222296    0.197705    00:05
6         0.195828    0.180353    00:05
7         0.180438    0.180277    00:05
8         0.172637    0.174684    00:05
9         0.166918    0.172907    00:05
10        0.162374    0.170295    00:05
11        0.158530    0.168603    00:05
12        0.156552    0.170809    00:05
13        0.153919    0.168128    00:05
14        0.151016    0.167631    00:05
15        0.149203    0.161024    00:05
16        0.150092    0.161494    00:05
17        0.149340    0.160975    00:05
18        0.146962    0.161289    00:05
19        0.145652    0.157666    00:05
20        0.145433    0.164793    00:05
21        0.142594    0.154554    00:05
22        0.140053    0.157742    00:05
23        0.139358    0.158078    00:05
24        0.137804    0.153264    00:05
25        0.134718    0.155475    00:05
26        0.134521    0.149421    00:05
27        0.132744    0.150203    00:05
28        0.131227    0.149360    00:05
29        0.130126    0.148933    00:05
30        0.128968    0.147128    00:05
31        0.127951    0.147784    00:05
32        0.125435    0.147764    00:05
33        0.124037    0.147521    00:05
34        0.121881    0.146251    00:05
35        0.120654    0.144552    00:05
36        0.119785    0.145585    00:05
37        0.117905    0.145567    00:05
38        0.116068    0.144135    00:05
39        0.115723    0.143970    00:05
40        0.113486    0.145618    00:05
41        0.112851    0.146409    00:05
42        0.111916    0.146000    00:05
43        0.111683    0.144577    00:05
44        0.110287    0.144756    00:05
45        0.108708    0.146382    00:05
46        0.108141    0.145790    00:05
47        0.107132    0.145887    00:05
48        0.107637    0.145578    00:05
49        0.107641    0.145977    00:05
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         0.975295    #na#        00:12
1         3.483037    #na#        00:09     ██████████████████████████-----------------------| 71.43% [40/56 00:09<00:03 2.6759]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.882730    0.709399    00:15
1         0.785318    0.615371    00:15
2         0.647460    0.414986    00:14
3         0.434618    0.225751    00:15
4         0.284569    0.210271    00:15
5         0.222527    0.183020    00:15
6         0.192734    0.194020    00:15
7         0.179035    0.171500    00:15
8         0.170434    0.178501    00:15
9         0.164484    0.171807    00:15
10        0.160386    0.166453    00:15
11        0.157884    0.172881    00:15
12        0.154116    0.163847    00:15
13        0.151848    0.158005    00:15
14        0.148154    0.161383    00:15
15        0.145533    0.161106    00:15
16        0.144505    0.164936    00:15
17        0.142807    0.157060    00:15
18        0.140800    0.155352    00:15
19        0.139298    0.153776    00:15
20        0.137638    0.153567    00:15
21        0.135515    0.151739    00:15
22        0.133016    0.152648    00:15
23        0.131611    0.148301    00:15
24        0.129408    0.148905    00:15
25        0.128652    0.151987    00:15
26        0.127631    0.149037    00:15
27        0.126510    0.149644    00:15
28        0.124587    0.145876    00:15
29        0.121944    0.147230    00:15
30        0.120306    0.148624    00:15
31        0.119272    0.145183    00:15
32        0.118310    0.147562    00:15
33        0.116352    0.144886    00:15
34        0.114145    0.146057    00:15
35        0.111512    0.149612    00:15
36        0.109542    0.147504    00:15
37        0.107695    0.146952    00:15
38        0.106091    0.146066    00:15
39        0.104153    0.150628    00:15
40        0.102834    0.148205    00:15
41        0.100208    0.147257    00:15
42        0.098963    0.151951    00:15
43        0.098144    0.149234    00:15
44        0.095872    0.149474    00:15
45        0.096361    0.149931    00:15
46        0.094751    0.150995    00:15
47        0.094959    0.150520    00:15
48        0.095346    0.150425    00:15
49        0.094243    0.150648    00:15
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
naive
Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         3.739204    #na#        00:06     ███████████████████████████----------------------| 73.28% [96/131 00:06<00:02 1.7760]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.757733    0.566511    00:11
1         0.479453    0.281779    00:12
2         0.199989    0.119053    00:12
3         0.120748    0.087452    00:12
4         0.096329    0.087424    00:12
5         0.083845    0.099613    00:12
6         0.077454    0.076453    00:12
7         0.072122    0.068189    00:12
8         0.070853    0.074173    00:12
9         0.069367    0.068340    00:12
10        0.066389    0.064145    00:12
11        0.066562    0.064966    00:12
12        0.065829    0.063832    00:12
13        0.063787    0.063945    00:12
14        0.062820    0.066209    00:12
15        0.061528    0.061711    00:12
16        0.061030    0.062873    00:12
17        0.059787    0.062139    00:12
18        0.058989    0.060572    00:12
19        0.057493    0.057927    00:12
20        0.058860    0.062134    00:12
21        0.057530    0.056387    00:12
22        0.056265    0.057103    00:12
23        0.054875    0.056246    00:12
24        0.055171    0.056295    00:12
25        0.053805    0.054761    00:12
26        0.054557    0.057947    00:12
27        0.054063    0.056699    00:12
28        0.053042    0.055129    00:12
29        0.052717    0.055386    00:12
30        0.052682    0.055442    00:13
31        0.051482    0.055478    00:12
32        0.053284    0.054213    00:12
33        0.050297    0.054016    00:12
34        0.049505    0.054993    00:12
35        0.050190    0.053319    00:12
36        0.047442    0.053298    00:12
37        0.046254    0.053926    00:12
38        0.047594    0.054097    00:12
39        0.047413    0.053923    00:12
40        0.045000    0.053579    00:12
41        0.045675    0.053924    00:13
42        0.044602    0.053985    00:12
43        0.042990    0.053892    00:12
44        0.043671    0.053723    00:12
45        0.045262    0.053841    00:33
46        0.043488    0.054133    01:04
47        0.042815    0.054205    00:27
48        0.042853    0.054154    00:11
49        0.043970    0.054190    00:11
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         3.065293    #na#        00:20     ████████████████████████████---------------------| 74.81% [98/131 00:20<00:06 2.2773]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.766062    0.577596    00:31
1         0.476766    0.285703    00:34
2         0.198116    0.119536    00:34
3         0.114591    0.088898    00:33
4         0.089676    0.084148    00:34
5         0.077330    0.078523    00:33
6         0.067742    0.062828    00:34
7         0.063432    0.064573    00:34
8         0.062411    0.072100    00:34
9         0.061828    0.058556    00:34
10        0.058956    0.055444    00:34
11        0.056870    0.055682    00:34
12        0.056875    0.057237    00:34
13        0.057111    0.052844    00:34
14        0.057592    0.065616    00:34
15        0.053652    0.057773    00:34
16        0.053431    0.052083    00:34
17        0.053234    0.052097    00:34
18        0.052194    0.051383    00:34
19        0.051650    0.051037    00:34
20        0.050884    0.050063    00:34
21        0.048724    0.051727    00:34
22        0.049378    0.051191    00:34
23        0.050269    0.051545    00:34
24        0.049791    0.048437    00:34
25        0.048504    0.049891    00:34
26        0.048147    0.048436    00:34
27        0.046094    0.048805    00:34
28        0.046565    0.049726    00:34
29        0.045480    0.048700    00:34
30        0.044827    0.047855    00:34
31        0.043875    0.049186    00:34
32        0.041559    0.048246    00:34
33        0.041839    0.048784    00:34
34        0.042659    0.048902    00:34
35        0.042194    0.048217    00:34
36        0.039572    0.047374    00:34
37        0.040518    0.047514    00:34
38        0.037709    0.050020    00:34
39        0.039503    0.048081    00:34
40        0.038634    0.049105    00:34
41        0.037032    0.048512    00:34
42        0.037645    0.048620    00:34
43        0.035734    0.049810    00:34
44        0.035790    0.049726    00:34
45        0.036463    0.049349    00:34
46        0.035760    0.049850    00:34
47        0.034799    0.050048    00:34
48        0.035392    0.049808    00:34
49        0.036031    0.049869    00:34
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
fastai_inception1d
fastai_resnet1d_wang
naive

 ### 1. PTB-XL: all statements

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| inception1d | 0.926(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | 0.925(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| resnet1d_wang | 0.920(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

 ### 2. PTB-XL: diagnostic statements

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| resnet1d_wang | 0.934(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| inception1d | 0.932(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | -- | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

 ### 3. PTB-XL: Diagnostic subclasses

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| resnet1d_wang | 0.934(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| inception1d | 0.923(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | -- | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

 ### 4. PTB-XL: Diagnostic superclasses

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| resnet1d_wang | 0.930(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| inception1d | 0.918(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | -- | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

 ### 5. PTB-XL: Form statements

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| inception1d | 0.895(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| resnet1d_wang | 0.877(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | -- | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

 ### 6. PTB-XL: Rhythm statements

| Model | AUC &darr; | paper/source | code |
|---:|:---|:---|:---|
| inception1d | 0.952(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| resnet1d_wang | 0.936(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| xresnet1d101 | -- | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|

Training from scratch...
model: fastai_resnet1d_wang
epoch     train_loss  valid_loss  time
0         1.045566    #na#        00:03
1         0.760530    #na#        00:02
2         3.147844    #na#        00:00     -------------------------------------------------| 26.19% [11/42 00:00<00:02 2.1016]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.874571    0.627891    00:04
1         0.778476    0.567265    00:04
2         0.689156    0.488549    00:04
3         0.558367    0.324242    00:04
4         0.419228    0.256740    00:04
5         0.326046    0.241432    00:04
6         0.266752    0.204584    00:04
7         0.230367    0.206222    00:04
8         0.207980    0.190826    00:04
9         0.196597    0.180790    00:04
10        0.183928    0.178477    00:04
11        0.175668    0.196637    00:04
12        0.170595    0.170888    00:04
13        0.168447    0.164465    00:04
14        0.165030    0.164411    00:04
15        0.162865    0.178735    00:04
16        0.159696    0.172749    00:04
17        0.158282    0.164782    00:04
18        0.156122    0.164399    00:04
19        0.155554    0.164326    00:04
20        0.152171    0.159970    00:04
21        0.149774    0.172056    00:04
22        0.147240    0.160770    00:04
23        0.146681    0.151634    00:04
24        0.143854    0.153485    00:04
25        0.142513    0.152131    00:04
26        0.140396    0.145125    00:04
27        0.136701    0.148567    00:04
28        0.134448    0.160049    00:04
29        0.132553    0.153537    00:04
30        0.130055    0.147741    00:04
31        0.129550    0.168651    00:04
32        0.124822    0.154498    00:04
33        0.122793    0.147054    00:04
34        0.121130    0.150087    00:04
35        0.117456    0.144206    00:04
36        0.115223    0.144137    00:04
37        0.113012    0.144889    00:04
38        0.109527    0.146156    00:04
39        0.106077    0.145135    00:04
40        0.102623    0.145973    00:04
41        0.099166    0.148038    00:04
42        0.097295    0.146651    00:04
43        0.095921    0.147581    00:04
44        0.093734    0.147822    00:04
45        0.091985    0.149813    00:04
46        0.090458    0.149415    00:04
47        0.089769    0.148911    00:04
48        0.089068    0.149313    00:04
49        0.090739    0.148167    00:04
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
model: fastai_resnet1d_wang
aggregating predictions...
Training from scratch...
model: fastai_inception1d
epoch     train_loss  valid_loss  time
0         0.997230    #na#        00:08
1         0.726547    #na#        00:09
2         2.990273    #na#        00:02     -------------------------------------------------| 23.81% [10/42 00:02<00:07 2.1252]
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  time
0         0.849653    0.648991    00:11
1         0.757842    0.556227    00:12
2         0.657956    0.458930    00:12
3         0.533010    0.302263    00:12
4         0.392206    0.237252    00:12
5         0.296595    0.209842    00:12
6         0.242764    0.198545    00:12
7         0.210854    0.196508    00:12
8         0.193296    0.202029    00:12
9         0.183468    0.168801    00:12
10        0.172199    0.171132    00:12
11        0.165706    0.174273    00:12
12        0.162674    0.178950    00:12
13        0.157520    0.174671    00:12
14        0.154756    0.159838    00:12
15        0.152337    0.157335    00:12
16        0.151312    0.163932    00:12
17        0.146781    0.165674    00:12
18        0.146531    0.152357    00:12
19        0.144612    0.160545    00:12
20        0.141996    0.174401    00:12
21        0.139606    0.157406    00:12
22        0.136843    0.153598    00:12
23        0.134298    0.144478    00:12
24        0.134821    0.152379    00:12
25        0.132408    0.149998    00:12
26        0.129938    0.150986    00:12
27        0.125763    0.145230    00:12
28        0.122995    0.147847    00:12
29        0.121888    0.150709    00:12
30        0.118179    0.150044    00:12
31        0.115115    0.153207    00:12
32        0.110274    0.150704    00:12
33        0.107620    0.147663    00:12
34        0.103367    0.148861    00:12
35        0.101025    0.152881    00:12
36        0.096588    0.146968    00:12
37        0.091761    0.154929    00:12
38        0.088648    0.147502    00:12
39        0.084464    0.155021    00:12
40        0.081851    0.150862    00:12
41        0.078422    0.152339    00:12
42        0.074183    0.155724    00:12
43        0.071207    0.159646    00:12
44        0.067601    0.156734    00:12
45        0.066713    0.159363    00:12
46        0.065090    0.156465    00:12
47        0.065123    0.157400    00:12
48        0.063875    0.158389    00:12
49        0.063619    0.157582    00:12
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
model: fastai_inception1d
aggregating predictions...
ensemble
optimize thresholds with respect to G_beta
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:40<00:00,  4.52s/it]
fastai_inception1d
optimize thresholds with respect to G_beta
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:39<00:00,  4.44s/it]
fastai_resnet1d_wang
optimize thresholds with respect to G_beta
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:39<00:00,  4.35s/it]
naive
optimize thresholds with respect to G_beta
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:38<00:00,  4.27s/it]
| Model | AUC &darr; |  F_beta=2 | G_beta=2 | paper/source | code |
|---:|:---|:---|:---|:---|:---|
| resnet1d_wang | 0.962(00) | 0.806(00) | 0.598(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
| inception1d | 0.958(00) | 0.814(00) | 0.605(00) | [our work](https://arxiv.org/abs/2004.13701) | [this repo](https://github.com/helme/ecg_ptbxl_benchmarking/)|
